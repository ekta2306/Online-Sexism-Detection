{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsV6Sfrhzs67q4+lsdgYi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekta2306/Online-Sexism-Detection/blob/main/HateXplain_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install ekphrasis"
      ],
      "metadata": {
        "id": "biL4TR6AkfPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install plotly==4.5.4"
      ],
      "metadata": {
        "id": "jdLFx9XDklPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.2.1"
      ],
      "metadata": {
        "id": "GSb4W6Vgkn1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ekphrasis"
      ],
      "metadata": {
        "id": "OuMaYcgAkqXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "id": "R68YM3r8ksux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ],
      "metadata": {
        "id": "7IZQBACakv0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "id": "CIM91GockxtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_text(texts,i,j):\n",
        "    for u in range(i,j):\n",
        "        print(texts[u])\n",
        "        print()"
      ],
      "metadata": {
        "id": "XBqZ_ufIk0WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train_new.csv',delimiter=',',encoding='utf-8')  # train set\n",
        "print(list(df.columns.values)) #file header"
      ],
      "metadata": {
        "id": "DNR3H6QSk2jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_array = df[\"tweet\"]\n",
        "labels = df[\"class\"]\n",
        "original = text_array"
      ],
      "metadata": {
        "id": "T2JZwkAdk6bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Afg-barllEYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validate = pd.read_csv('/content/val.csv',delimiter=',',encoding='utf-8') # validation set"
      ],
      "metadata": {
        "id": "47QmX98vlGTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/test_new.csv',delimiter=',',encoding='utf-8') # test set"
      ],
      "metadata": {
        "id": "D37CdLZylIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if need to change\n",
        "text_array_val = df_validate[\"tweet\"]\n",
        "labels_val = df_validate[\"class\"]"
      ],
      "metadata": {
        "id": "e3LisRqOlMr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels_val)"
      ],
      "metadata": {
        "id": "AIrDG1JZlQea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_text = df_test[\"tweet\"]\n",
        "labels_test_set = df_test[\"class\"]"
      ],
      "metadata": {
        "id": "yhu2RDMhlSha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_website(text):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\b\\w+\\.(com|co|net)\\b', re.IGNORECASE)\n",
        "    return \" \".join([\"\" if pattern.search(word) else word for word in text.split()])\n",
        "\n",
        "# Clean training set\n",
        "text_array = text_array.apply(remove_website)\n",
        "print_text(text_array, 0, 10)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Clean validation set\n",
        "text_array_val = text_array_val.apply(remove_website)\n",
        "print_text(text_array_val, 0, 10)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Clean test set\n",
        "test_set_text = test_set_text.apply(remove_website)\n",
        "print_text(test_set_text, 0, 10)\n"
      ],
      "metadata": {
        "id": "-nM8VMYblWXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for chat word conversion\n",
        "f = open(\"/content/slang.txt\", \"r\")\n",
        "chat_words_str = f.read()\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "metadata": {
        "id": "rKMsYVxhlXsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat word conversion\n",
        "# Training set\n",
        "text_array = text_array.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_array,0,10)\n",
        "print_text(original,0,10)\n",
        "\n",
        "print(\"********************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_array_val = text_array_val.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_array_val,0,10)\n",
        "\n",
        "print(\"********************************************************************************\")\n",
        "\n",
        "# Test set\n",
        "test_set_text = test_set_text.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(test_set_text,0,10)"
      ],
      "metadata": {
        "id": "acbTdAd4ld7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check code in other copy\n",
        "import re\n",
        "\n",
        "# Define a dictionary of emoticons and their corresponding meanings\n",
        "EMOTICONS = {\n",
        "    ':)': 'smile',\n",
        "    ':-)': 'smile',\n",
        "    ':(': 'sad',\n",
        "    ':-(': 'sad',\n",
        "    ':D': 'laugh',\n",
        "    '<3': 'love'\n",
        "}\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    # Iterate over each emoticon in the dictionary\n",
        "    for emot, meaning in EMOTICONS.items():\n",
        "        # Replace the emoticon with its corresponding meaning in the text\n",
        "        text = re.sub(r'(?i)' + re.escape(emot), meaning, text)\n",
        "    return text\n",
        "\n",
        "# Test the emoticon conversion function\n",
        "text = \"Hello :-) :-(\"\n",
        "text = convert_emoticons(text)\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "GlIZ00mOlgsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emoticon conversion\n",
        "# Training set\n",
        "text_array = text_array.apply(lambda text: convert_emoticons(text))\n",
        "print_text(text_array,0,10)\n",
        "\n",
        "print(\"**********************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_array_val = text_array_val.apply(lambda text: convert_emoticons(text))\n",
        "print_text(text_array_val,0,10)\n",
        "\n",
        "print(\"**********************************************************************************\")\n",
        "\n",
        "# Test set\n",
        "test_set_text = test_set_text.apply(lambda text: convert_emoticons(text))\n",
        "print_text(test_set_text,0,10)"
      ],
      "metadata": {
        "id": "-KiqIS1nli9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUnction for removal of emoji\n",
        "import emoji\n",
        "\n",
        "def convert_emojis(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(\"_|-\",\" \",text)\n",
        "    return text\n",
        "\n",
        "# Training set\n",
        "text_array = text_array.apply(lambda text: convert_emojis(text))\n",
        "print_text(text_array,0,10)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_array_val = text_array_val.apply(lambda text: convert_emojis(text))\n",
        "print_text(text_array_val,0,10)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Test set\n",
        "test_set_text = test_set_text.apply(lambda text: convert_emojis(text))\n",
        "print_text(test_set_text,0,10)"
      ],
      "metadata": {
        "id": "AvA-qxxblltN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekphrasis pipe for text pre-processing\n",
        "def ekphrasis_pipe(sentence):\n",
        "    cleaned_sentence = \" \".join(text_processor.pre_process_doc(sentence))\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Training set\n",
        "text_array = text_array.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Training set completed.......\")\n",
        "#Validation set\n",
        "text_array_val = text_array_val.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Validation set completed.......\")\n",
        "#Test set\n",
        "test_set_text = test_set_text.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "4k3AZKX3ln3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unnecessary punctuations\n",
        "PUNCT_TO_REMOVE = \"\\\"$%&'()+,-./;=[\\]^_`{|}~\"\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "# Training set\n",
        "text_array = text_array.apply(lambda text: remove_punctuation(text))\n",
        "print_text(text_array,0,10)\n",
        "\n",
        "print(\"********************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_array_val = text_array_val.apply(lambda text: remove_punctuation(text))\n",
        "print_text(text_array_val,0,10)\n",
        "\n",
        "print(\"********************************************************************\")\n",
        "\n",
        "# Test set\n",
        "test_set_text = test_set_text.apply(lambda text: remove_punctuation(text))\n",
        "print_text(test_set_text,0,10)"
      ],
      "metadata": {
        "id": "OIvBv1iClrNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of each label in dataset\n",
        "from collections import Counter\n",
        "\n",
        "# Printing training set counts for analysis\n",
        "print(\"Elements: \",set(labels))\n",
        "print(\"Length: \",len(labels))\n",
        "print(Counter(labels))\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Printing validation set counts for analysis\n",
        "print(\"Elements: \",set(labels_val))\n",
        "print(\"Length: \",len(labels_val))\n",
        "print(Counter(labels_val))\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Printing Test set counts for analysis\n",
        "print(\"Elements: \",set(labels_test_set))\n",
        "print(\"Length: \",len(labels_test_set))\n",
        "print(Counter(labels_test_set))"
      ],
      "metadata": {
        "id": "QB0y8xvwls2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = []\n",
        "Y_vali = []\n",
        "Y_test_set = []\n",
        "\n",
        "# Training set\n",
        "for i in range(0, len(labels)):\n",
        "    if labels[i] == 0:\n",
        "        Y.append(0)\n",
        "    elif labels[i] == 1:\n",
        "        Y.append(1)\n",
        "    elif labels[i] == 2:\n",
        "        Y.append(2)\n",
        "\n",
        "# Validation set\n",
        "for i in range(0, len(labels_val)):\n",
        "    if labels_val[i] == 0:\n",
        "        Y_vali.append(0)\n",
        "    elif labels_val[i] == 1:\n",
        "        Y_vali.append(1)\n",
        "    elif labels_val[i] == 2:\n",
        "        Y_vali.append(2)\n",
        "\n",
        "# Test set\n",
        "for i in range(0, len(labels_test_set)):\n",
        "    if labels_test_set[i] == 0:\n",
        "        Y_test_set.append(0)\n",
        "    elif labels_test_set[i] == 1:\n",
        "        Y_test_set.append(1)\n",
        "    elif labels_test_set[i] == 2:\n",
        "        Y_test_set.append(2)\n"
      ],
      "metadata": {
        "id": "agSk-BkZnKra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "id": "J6qK8BLqnut-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y)"
      ],
      "metadata": {
        "id": "_4PIAv78nvS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels_val)"
      ],
      "metadata": {
        "id": "nxMyHjlqnxCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_vali)"
      ],
      "metadata": {
        "id": "9TNl0u-AnzKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying train set\n",
        "X = np.asarray(list(text_array))\n",
        "Y = np.asarray(list(Y))\n",
        "print(type(X))\n",
        "print(type(Y))\n",
        "print(np.shape(X),np.shape(Y))\n",
        "\n",
        "# Verifying validation set\n",
        "X_val = np.asarray(list(text_array_val))\n",
        "Y_vali = np.asarray(list(Y_vali))\n",
        "print(type(X_val))\n",
        "print(type(Y_vali))\n",
        "print(np.shape(X_val),np.shape(Y_vali))\n",
        "\n",
        "# Verifying test set\n",
        "X_test_set = np.asarray(list(test_set_text))\n",
        "Y_test_set = np.asarray(list(Y_test_set))\n",
        "print(type(X_test_set))\n",
        "print(type(Y_test_set))\n",
        "print(np.shape(X_test_set),np.shape(Y_test_set))"
      ],
      "metadata": {
        "id": "TMTjmigXn2nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bL0KcciFn5EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to one hot vectors\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)] #u[Y] helps to index each element of Y index at u. U here is a class array\n",
        "    return Y"
      ],
      "metadata": {
        "id": "TkgRVOwqn7LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_oh_train = convert_to_one_hot(np.array(Y), C = 3)\n",
        "Y_oh_val = convert_to_one_hot(np.array(Y_vali), C = 3)\n",
        "Y_oh_test_set = convert_to_one_hot(np.array(Y_test_set), C = 3)"
      ],
      "metadata": {
        "id": "TRBt9htnn9DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Kb1-zcr8n-1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import RobertaTokenizerFast, TFRobertaModel, TFBertModel, BertTokenizerFast, ElectraTokenizerFast, TFElectraModel, AlbertTokenizerFast, TFAlbertModel, XLNetTokenizerFast, TFXLNetModel, MPNetTokenizerFast, TFMPNetModel\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import RepeatVector, Concatenate, Dense, Activation, Dot, BatchNormalization, Dropout\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "uheWkNoSoEq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "njXOTRzAoGgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if a GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "\n",
        "# List available GPU devices\n",
        "print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n"
      ],
      "metadata": {
        "id": "_6sW2rnNoId-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "9KmNtRaroMbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(X)\n",
        "X_val = list(X_val)\n",
        "X_test_set = list(X_test_set)"
      ],
      "metadata": {
        "id": "6a1TxOd-owVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_val)"
      ],
      "metadata": {
        "id": "jx2IrGZMozQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_vali)"
      ],
      "metadata": {
        "id": "EU9rcA_Koz29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train_x, model_val_x, Y_train, Y_val = train_test_split(X, Y, test_size=0.05, random_state=44)"
      ],
      "metadata": {
        "id": "8bKMBwrno1TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(model_train_x, max_length=100, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "val_encodings = tokenizer(model_val_x, max_length=100, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "test_encodings = tokenizer(X_val, max_length=100, truncation=True, padding=\"max_length\", return_tensors='tf')"
      ],
      "metadata": {
        "id": "oxoUUjDmo3QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_encodings = tokenizer(X, max_length=100, truncation=True, padding=\"max_length\", return_tensors='tf')"
      ],
      "metadata": {
        "id": "H5NXdZdAo58W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "q0_4M0sMo6v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from transformers import TFRobertaModel\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def Offense_classifier(input_shape, dropout_rate=0.3, dense_units=256):\n",
        "    roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    input_ids = layers.Input(shape=input_shape, dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = layers.Input(shape=input_shape, dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    def roberta_call(inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        roberta_output = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return roberta_output.pooler_output\n",
        "\n",
        "    roberta_embeddings = layers.Lambda(roberta_call, output_shape=(768,))([input_ids, attention_mask])\n",
        "\n",
        "    x = layers.BatchNormalization()(roberta_embeddings)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    x = layers.Dense(128, activation='elu')(x)\n",
        "    x = layers.Dense(32, activation='elu')(x)\n",
        "    x = layers.Dense(3, activation='elu')(x)\n",
        "    x = layers.Dense(32, activation='elu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation='elu')(x)\n",
        "\n",
        "    output = layers.Dense(3, activation='softmax')(x)  # Updated for 3-class classification\n",
        "\n",
        "    return models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Use MirroredStrategy for distributed training (optional)\n",
        "with tf.distribute.MirroredStrategy().scope():\n",
        "    model = Offense_classifier((100,))  # Sequence length of 100 tokens\n",
        "\n",
        "    optimizer = Adam(learning_rate=3e-5)\n",
        "    loss_fun = tf.keras.losses.SparseCategoricalCrossentropy()  # Suitable for class indices 0/1/2\n",
        "    metric = ['accuracy']\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "WbcEKmrypIXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "id": "YNy3LHZ8pJVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class EvaluationMetric(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, trial_encodings, trial_masks, Y_vali):\n",
        "        super(EvaluationMetric, self).__init__()\n",
        "        self.trial_encodings = trial_encodings\n",
        "        self.trial_masks = trial_masks\n",
        "        self.Y_vali = Y_vali\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(\"\\nTraining...\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"\\nEvaluating...\")\n",
        "        trial_prediction = self.model.predict([self.trial_encodings, self.trial_masks])\n",
        "\n",
        "        # Use argmax to get class labels (0, 1, 2)\n",
        "        pred = np.argmax(trial_prediction, axis=1)\n",
        "\n",
        "        # Show classification report\n",
        "        print(classification_report(self.Y_vali, pred, digits=3))\n",
        "\n",
        "# Usage:\n",
        "evaluation_metric = EvaluationMetric(\n",
        "    test_encodings[\"input_ids\"],\n",
        "    test_encodings[\"attention_mask\"],\n",
        "    Y_vali\n",
        ")\n"
      ],
      "metadata": {
        "id": "nRe4wWZvpNIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "id": "ZvbmUC5Npc2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFRobertaModel\n",
        "\n",
        "roberta_model = TFRobertaModel.from_pretrained(\"roberta-base\", from_pt=True)\n",
        "# \"from_pt=True\" converts a PyTorch model to TensorFlow.\n"
      ],
      "metadata": {
        "id": "PJnKFZtapex6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n"
      ],
      "metadata": {
        "id": "F_AnDjjupfsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "with strategy.scope():\n",
        "    # Define and compile your model\n",
        "    model = Offense_classifier(input_shape=(100,), dropout_rate=0.3, dense_units=256)\n",
        "\n",
        "    optimizer = Adam(learning_rate=3e-5)\n",
        "    loss_fun = tf.keras.losses.SparseCategoricalCrossentropy()  # For integer labels 0,1,2\n",
        "    metrics = ['accuracy']\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metrics)\n"
      ],
      "metadata": {
        "id": "QwM6JAJhpg_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count samples per class\n",
        "class_counts = np.bincount(Y_train)\n",
        "total = len(Y_train)\n",
        "print(f'Examples:\\n    Total: {total}\\n    Per-class counts: {class_counts}\\n')\n",
        "\n",
        "# Calculate weights: inverse frequency or max ratio\n",
        "maxi = max(class_counts)\n",
        "class_weight = {i: (maxi / count if count > 0 else 1.0) for i, count in enumerate(class_counts)}\n",
        "\n",
        "for k, v in class_weight.items():\n",
        "    print(f'Weight for class {k}: {v:.2f}')\n"
      ],
      "metadata": {
        "id": "7Yfg9y0kpjZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Save best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',                     # File to save\n",
        "    monitor='val_accuracy',             # Metric to monitor\n",
        "    verbose=1,\n",
        "    save_best_only=True,                # Only keep the best\n",
        "    mode='max'                          # 'max' because higher accuracy is better\n",
        ")\n"
      ],
      "metadata": {
        "id": "z2XLaUtQqOvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x=[train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"]],\n",
        "    y=Y_train,\n",
        "    validation_data=([val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"]], Y_val),\n",
        "    callbacks=[evaluation_metric, checkpoint],\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    epochs=6,\n",
        "    class_weight=class_weight\n",
        ")\n"
      ],
      "metadata": {
        "id": "t6l688-vp5Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TAnuwh8Grcqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_encodings = tokenizer(X_test_set, max_length=100, truncation=True, padding=\"max_length\", return_tensors='tf')"
      ],
      "metadata": {
        "id": "k2pBiRZwrsKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = model.predict([test_set_encodings[\"input_ids\"], test_set_encodings[\"attention_mask\"]])"
      ],
      "metadata": {
        "id": "p6Keeuk8rtqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pred = []\n",
        "for i in range(len(X_test_set)):\n",
        "    num = answer[i]\n",
        "    predicted_class = np.argmax(num)  # Select class with highest probability\n",
        "    pred.append(predicted_class)\n"
      ],
      "metadata": {
        "id": "jyVYolIrru1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_mat = tf.math.confusion_matrix(labels=Y_test_set, predictions=pred)\n",
        "print(con_mat)"
      ],
      "metadata": {
        "id": "kek7a-PcrxSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iVqp-Uahryz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class_names = [\"Hate Speech\", \"Offensive Language\", \"Neither\"]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.set(font_scale=1.5)\n",
        "\n",
        "sns.heatmap(\n",
        "    con_mat,\n",
        "    annot=True,\n",
        "    cmap=plt.cm.viridis,\n",
        "    fmt='d',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    annot_kws={\"size\": 15}\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "peYmSpqrr0HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "BURhSmDutWdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(Y_test_set, pred, average='macro')"
      ],
      "metadata": {
        "id": "nL_maOFktWvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Y_test_set, pred, target_names=[\"Hate Speech\", \"Offensive Language\", \"Neither\"], digits=3))\n"
      ],
      "metadata": {
        "id": "tVQ7EjLStX4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_train = model.predict([cluster_encodings[\"input_ids\"], cluster_encodings[\"attention_mask\"]])"
      ],
      "metadata": {
        "id": "-QnR8Kl4tY_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_train"
      ],
      "metadata": {
        "id": "Rwc6dlZMtaRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "RNxr55b0thvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 neuron output\n",
        "model.layers[-6].name"
      ],
      "metadata": {
        "id": "F7Ep0_t-tibf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all layer names\n",
        "for layer in model.layers:\n",
        "    print(layer.name)\n"
      ],
      "metadata": {
        "id": "Ax1zb43auefC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Find the output tensor of the desired layer by name\n",
        "intermediate_output = [layer.output for layer in model.layers if layer.name == 'dense_12'][0]\n",
        "\n",
        "# Create a new model that ends at that intermediate layer\n",
        "cluster_dense_3 = Model(inputs=model.inputs, outputs=intermediate_output)\n"
      ],
      "metadata": {
        "id": "giEcvP07t1fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_3 = cluster_dense_3.predict([cluster_encodings[\"input_ids\"], cluster_encodings[\"attention_mask\"]])\n"
      ],
      "metadata": {
        "id": "WPAnuy0Mt2Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_train = []\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    score = (\n",
        "        float(answer_train[i][0]) if isinstance(answer_train[i], (list, np.ndarray))\n",
        "        else float(answer_train[i])\n",
        "    )\n",
        "\n",
        "    if 0 < score <= 0.3:\n",
        "        pred_train.append(0)  # Hate Speech\n",
        "    elif 0.7 <= score < 1:\n",
        "        pred_train.append(1)  # Offensive Language\n",
        "    elif 0.3 < score < 0.7:\n",
        "        pred_train.append(2)  # Neither\n",
        "    else:\n",
        "        pred_train.append(2)  # For exact 0 or 1 (edge cases)\n",
        "\n",
        "print(\"Sample of predicted labels:\", pred_train[:10])\n"
      ],
      "metadata": {
        "id": "zSVtT9U_vwz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "flag = []\n",
        "x_ = []\n",
        "y_ = []\n",
        "z_ = []\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    x_.append(cluster_3[i][1])\n",
        "    y_.append(cluster_3[i][0])\n",
        "    z_.append(cluster_3[i][2])\n",
        "\n",
        "    score_raw = answer_train[i]\n",
        "\n",
        "    # Extract first float if it's an array or list\n",
        "    score = (\n",
        "        float(score_raw[0])\n",
        "        if isinstance(score_raw, (list, np.ndarray))\n",
        "        else float(score_raw)\n",
        "    )\n",
        "\n",
        "    # Class assignment based on thresholds\n",
        "    if 0 < score <= 0.3:\n",
        "        flag.append(0)  # Hate Speech\n",
        "    elif 0.7 <= score < 1:\n",
        "        flag.append(1)  # Offensive Language\n",
        "    elif 0.3 < score < 0.7:\n",
        "        flag.append(2)  # Neither\n",
        "    else:\n",
        "        flag.append(2)  # For exact 0 or 1\n",
        "\n",
        "from collections import Counter\n",
        "print(\"Class distribution:\", Counter(flag))\n"
      ],
      "metadata": {
        "id": "aCviYRBnt33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(flag)"
      ],
      "metadata": {
        "id": "dmfLyUPAvF5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_mat = tf.math.confusion_matrix(labels=Y, predictions=pred_train)\n",
        "print(con_mat)"
      ],
      "metadata": {
        "id": "Fw8HSn1HvHsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Map class indices to new labels\n",
        "pred_colour = []\n",
        "for i in range(len(flag)):\n",
        "    if flag[i] == 0:\n",
        "        pred_colour.append(\"Hate Speech\")\n",
        "    elif flag[i] == 1:\n",
        "        pred_colour.append(\"Offensive Language\")\n",
        "    elif flag[i] == 2:\n",
        "        pred_colour.append(\"Neither\")\n",
        "    else:\n",
        "        pred_colour.append(\"Unknown\")  # Optional fallback\n",
        "\n",
        "# Create DataFrame for Plotly\n",
        "test_df = pd.DataFrame({'x': x_, 'y': y_, 'z': z_, 'Labels': pred_colour})\n",
        "\n",
        "# Create 3D scatter plot\n",
        "fig = px.scatter_3d(test_df, x='x', y='y', z='z', color='Labels')\n",
        "\n",
        "# Style settings\n",
        "fig.update_traces(\n",
        "    marker={\n",
        "        'size': 1,\n",
        "        'opacity': 0.7,\n",
        "        'colorscale': 'Oryel',\n",
        "    }\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend={'itemsizing': 'constant', 'font_size': 18},\n",
        "    font_size=15,\n",
        "    scene_aspectmode='cube',\n",
        "    width=750,\n",
        "    height=500,\n",
        "    margin=dict(l=0, r=0, b=0, t=0)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "eZCvNqUtvzLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Map numerical predictions to labels\n",
        "pred_colour = []\n",
        "for i in range(len(pred_train)):\n",
        "    if pred_train[i] == 0:\n",
        "        pred_colour.append(\"Hate Speech\")\n",
        "    elif pred_train[i] == 1:\n",
        "        pred_colour.append(\"Offensive Language\")\n",
        "    elif pred_train[i] == 2:\n",
        "        pred_colour.append(\"Neither\")\n",
        "    else:\n",
        "        pred_colour.append(\"Unknown\")\n",
        "\n",
        "# Create DataFrame for 3D visualization\n",
        "test_df = pd.DataFrame({'X': x_, 'Y': y_, 'Z': z_, 'Labels': pred_colour})\n",
        "\n",
        "# Generate the 3D scatter plot\n",
        "fig = px.scatter_3d(test_df, x='X', y='Y', z='Z', color='Labels')\n",
        "\n",
        "# Customize marker and layout\n",
        "fig.update_traces(\n",
        "    marker={\n",
        "        'size': 1,\n",
        "        'opacity': 1,\n",
        "        'colorscale': 'rainbow',\n",
        "    }\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend={'itemsizing': 'constant'},\n",
        "    font_size=14,\n",
        "    scene_aspectmode='cube',\n",
        "    width=850,\n",
        "    height=750,\n",
        "    margin=dict(l=0, r=0, b=0, t=0)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "rhqOuzSmwX5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-8].name"
      ],
      "metadata": {
        "id": "Nj2EY3eUwpWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Safely get the output of the layer using index (-8) or name if you know it\n",
        "intermediate_output = model.layers[-8].output\n",
        "\n",
        "# Create the intermediate model\n",
        "cluster_bert = Model(inputs=model.inputs, outputs=intermediate_output)\n",
        "\n",
        "# Predict using the strategy scope\n",
        "with strategy.scope():\n",
        "    cl_bert = cluster_bert.predict([cluster_encodings[\"input_ids\"], cluster_encodings[\"attention_mask\"]])\n"
      ],
      "metadata": {
        "id": "tRPafQm7ws0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cl_bert)"
      ],
      "metadata": {
        "id": "fVdjp08IwwBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag_bert = []\n",
        "x_bert = []\n",
        "y_bert = []\n",
        "z_bert = []\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    # Append cluster coordinates (assuming cl_bert[i] = [x, y, z])\n",
        "    x_bert.append(cl_bert[i][0])\n",
        "    y_bert.append(cl_bert[i][1])\n",
        "    z_bert.append(cl_bert[i][2])\n",
        "\n",
        "    # Safely extract scalar score (handles if answer_train[i] is array-like)\n",
        "    score = (\n",
        "        float(answer_train[i][0]) if isinstance(answer_train[i], (list, np.ndarray))\n",
        "        else float(answer_train[i])\n",
        "    )\n",
        "\n",
        "    # Assign class based on thresholds\n",
        "    if 0 < score <= 0.28:\n",
        "        flag_bert.append(0)   # Hate Speech\n",
        "    elif 0.28 < score < 0.8:\n",
        "        flag_bert.append(2)   # Neither\n",
        "    elif 0.8 <= score < 1:\n",
        "        flag_bert.append(1)   # Offensive Language\n",
        "    else:\n",
        "        flag_bert.append(2)   # fallback to Neither (for edge cases 0 or 1)\n",
        "\n",
        "print(f\"Total points processed: {len(Y)}\")\n"
      ],
      "metadata": {
        "id": "QbwsCzPMw2wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Convert coordinates into feature matrix (NumPy array)\n",
        "X = np.array(list(zip(x_bert, y_bert, z_bert)))\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "y = np.array(flag_bert)\n",
        "\n",
        "# Split the data (stratify to maintain class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of the Random Forest model: {accuracy:.4f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mHNsrVZTw9O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Ensure cl_bert is a NumPy array\n",
        "import numpy as np\n",
        "cl_bert_np = np.array(cl_bert)\n",
        "\n",
        "# Fit KMeans with 3 clusters and random state 44\n",
        "kmeans_bert = KMeans(n_clusters=3, random_state=44)\n",
        "kmeans_bert.fit(cl_bert_np)\n",
        "\n",
        "# Predict cluster labels for cl_bert data\n",
        "y_kmeans_bert = kmeans_bert.predict(cl_bert_np)\n"
      ],
      "metadata": {
        "id": "04xWokkgxLZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 index values are sexist\n",
        "# 0 index values are not sexist\n",
        "# 2 index values are neutral\n",
        "\n",
        "# Count how many times flag_bert == 2 (neutral) and predicted cluster is 1\n",
        "count = 0\n",
        "for i in range(len(flag_bert)):\n",
        "    if flag_bert[i] == 2 and y_kmeans_bert[i] == 1:\n",
        "        count += 1\n",
        "print(\"Count of neutral samples predicted as cluster 1:\", count)\n",
        "\n",
        "# Swap cluster labels 0 and 1 in y_kmeans_bert, keep 2 as is\n",
        "for i in range(len(y_kmeans_bert)):\n",
        "    if y_kmeans_bert[i] == 0:\n",
        "        y_kmeans_bert[i] = 1\n",
        "    elif y_kmeans_bert[i] == 1:\n",
        "        y_kmeans_bert[i] = 0\n",
        "    # If it's 2, keep as 2 (neutral)\n",
        "\n",
        "# Recompute flag_bert based on answer_train thresholds and coordinates from cl_bert\n",
        "flag_bert = []\n",
        "x_bert = []\n",
        "y_bert = []\n",
        "z_bert = []\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    x_bert.append(cl_bert[i][0])\n",
        "    y_bert.append(cl_bert[i][1])\n",
        "    z_bert.append(cl_bert[i][2])\n",
        "\n",
        "    score = answer_train[i]\n",
        "\n",
        "    # Check if score is array-like and has more than 1 element\n",
        "    if hasattr(score, 'shape') and len(score) > 1:\n",
        "        # Example: take first element or the max\n",
        "        score = score[0]   # or score = np.max(score) or np.argmax(score)\n",
        "    else:\n",
        "        # If it's a scalar-like array of size 1, get the scalar\n",
        "        if hasattr(score, 'shape') and len(score) == 1:\n",
        "            score = score.item()\n",
        "\n",
        "    if 0 < score <= 0.3:\n",
        "        flag_bert.append(0)  # Not sexist\n",
        "    elif 0.7 <= score < 1:\n",
        "        flag_bert.append(1)  # Sexist\n",
        "    elif 0.3 < score < 0.7:\n",
        "        flag_bert.append(2)  # Neutral\n",
        "    else:\n",
        "        flag_bert.append(2)  # Default neutral\n",
        "\n",
        "print(\"Total samples processed:\", len(flag_bert))\n",
        "\n",
        "\n",
        "\n",
        "# Compute confusion matrix between true labels and predicted clusters\n",
        "import tensorflow as tf\n",
        "con_mat = tf.math.confusion_matrix(labels=flag_bert, predictions=y_kmeans_bert)\n",
        "print(\"Confusion matrix:\")\n",
        "print(con_mat.numpy())\n"
      ],
      "metadata": {
        "id": "TNJWX7J0xY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.classification_report(flag_bert, y_kmeans_bert, output_dict=False, digits=3))"
      ],
      "metadata": {
        "id": "FMC5f7gmxzR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "import numpy as np\n",
        "\n",
        "def match_clusters(true_labels, cluster_labels):\n",
        "    labels = np.unique(true_labels)\n",
        "    clusters = np.unique(cluster_labels)\n",
        "    label_map = {}\n",
        "\n",
        "    for cluster in clusters:\n",
        "        mask = cluster_labels == cluster\n",
        "        cluster_true_labels = true_labels[mask]\n",
        "\n",
        "        if len(cluster_true_labels) == 0:\n",
        "            # No samples in this cluster, assign a default label, say -1\n",
        "            label_map[cluster] = -1\n",
        "            continue\n",
        "\n",
        "        mode_result = mode(cluster_true_labels)\n",
        "\n",
        "        # Check if mode_result.mode is scalar or array\n",
        "        if hasattr(mode_result.mode, '__len__'):\n",
        "            most_common_label = mode_result.mode[0]\n",
        "        else:\n",
        "            most_common_label = mode_result.mode\n",
        "\n",
        "        label_map[cluster] = most_common_label\n",
        "\n",
        "    new_preds = np.array([label_map.get(cl, -1) for cl in cluster_labels])\n",
        "    return new_preds\n",
        "\n",
        "flag_bert_np = np.array(flag_bert)\n",
        "y_kmeans_bert_np = np.array(y_kmeans_bert)\n",
        "\n",
        "mapped_preds = match_clusters(flag_bert_np, y_kmeans_bert_np)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(flag_bert_np, mapped_preds, digits=3))\n"
      ],
      "metadata": {
        "id": "-YSf1bqrx-EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "VHimG-WGx151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers_bert = kmeans_bert.cluster_centers_"
      ],
      "metadata": {
        "id": "PALL59OAx2Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Assuming 3 clusters (offensive, non-offensive, neutral)\n",
        "cluster_labels = ['offensive', 'non_offensive', 'neutral']\n",
        "\n",
        "# Initialize dictionary to hold similarity scores for each cluster\n",
        "svns = {label: [] for label in cluster_labels}\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    sample = cl_bert[i]\n",
        "    for idx, label in enumerate(cluster_labels):\n",
        "        # cosine distance divided by 2, then converted to similarity (1 - value)\n",
        "        sim = 1 - (cosine(sample, centers_bert[idx]) / 2)\n",
        "        svns[label].append(sim)\n",
        "\n",
        "# Example: print lengths to verify all lists are complete\n",
        "for label in cluster_labels:\n",
        "    print(f\"Length of svns_{label}: {len(svns[label])}\")\n",
        "\n",
        "# Access individual lists:\n",
        "svns_off = svns['offensive']\n",
        "svns_noff = svns['non_offensive']\n",
        "svns_neu = svns['neutral']\n"
      ],
      "metadata": {
        "id": "JX3RWdrlyXDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "ZAOXXqqsy4bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_colour = []\n",
        "for i in range(len(Y)):\n",
        "    if y_kmeans_bert[i] == 2:\n",
        "        pred_colour.append(\"Neutral\")\n",
        "    elif y_kmeans_bert[i] == 1:\n",
        "        pred_colour.append(\"Not Offensive\")\n",
        "    elif y_kmeans_bert[i] == 0:\n",
        "        pred_colour.append(\"Offensive\")\n",
        "    else:\n",
        "        pred_colour.append(\"Unknown\")  # fallback just in case\n",
        "\n",
        "# Prepare dataframe for plotting\n",
        "test_df = pd.DataFrame({\n",
        "    'SVNS_Offensive': svns_off,\n",
        "    'SVNS_Not_Offensive': svns_noff,\n",
        "    'SVNS_Neutral': svns_neu,\n",
        "    'Labels': pred_colour\n",
        "})\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    test_df,\n",
        "    x='SVNS_Offensive',\n",
        "    y='SVNS_Not_Offensive',\n",
        "    z='SVNS_Neutral',\n",
        "    color='Labels',\n",
        "    color_discrete_map={\n",
        "        \"Offensive\": \"red\",\n",
        "        \"Not Offensive\": \"green\",\n",
        "        \"Neutral\": \"blue\",\n",
        "        \"Unknown\": \"gray\"\n",
        "    },\n",
        "    title=\"3D Scatter of SVNS Scores with Cluster Labels\"\n",
        ")\n",
        "\n",
        "fig.update_traces(marker=dict(size=3, opacity=0.8))\n",
        "fig.update_layout(\n",
        "    legend=dict(itemsizing='constant', font_size=16),\n",
        "    font_size=14,\n",
        "    scene_aspectmode='cube',\n",
        "    width=900,\n",
        "    height=700,\n",
        "    margin=dict(l=0, r=0, b=0, t=30)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "3jGHhklXy9w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num=len(Y)"
      ],
      "metadata": {
        "id": "G9v_9BdjzCSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred_krobert = []\n",
        "\n",
        "for i in range(num):\n",
        "    # Check which SVNS score is the highest for each sample\n",
        "    if svns_off[i] > svns_neu[i] + svns_noff[i]:\n",
        "        pred_krobert.append(0)  # Offensive\n",
        "    elif svns_noff[i] > svns_neu[i] and svns_noff[i] > svns_off[i]:\n",
        "        pred_krobert.append(1)  # Not Offensive\n",
        "    elif svns_neu[i] > svns_noff[i] and svns_neu[i] > svns_off[i]:\n",
        "        pred_krobert.append(2)  # Neutral\n",
        "    else:\n",
        "        pred_krobert.append(0)  # Default/fallback to Offensive\n",
        "\n",
        "print(classification_report(Y, pred_krobert, output_dict=False, digits=3))\n"
      ],
      "metadata": {
        "id": "Hl3jDUqmzFSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Prepare feature matrix and labels\n",
        "X = np.column_stack((svns_off, svns_noff, svns_neu))  # Shape: (num_samples, 3)\n",
        "y = np.array(Y)  # True labels\n",
        "\n",
        "# Split into train and test sets (e.g., 70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize and train logistic regression model with balanced class weights to handle imbalance\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "jyhWxF_lzcP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Stack the SVNS features as input vectors\n",
        "X = np.column_stack((svns_off, svns_noff, svns_neu))  # Shape (num_samples, 3)\n",
        "\n",
        "# True labels array (0 = Hate Speech, 1 = Offensive, 2 = Normal)\n",
        "y = np.array(Y)\n",
        "\n",
        "# Split into train and test sets, stratified to keep class proportions\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Logistic Regression with balanced class weights (important if classes are imbalanced)\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, multi_class='ovr')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Show detailed classification report\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "njDQ3XjFzu65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = np.column_stack((svns_off, svns_noff, svns_neu))\n",
        "y = np.array(Y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200, random_state=42, class_weight='balanced'\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "b6qYr4K2z7Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "intjs0bB0Br-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}